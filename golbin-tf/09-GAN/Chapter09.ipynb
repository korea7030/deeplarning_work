{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN의 정의 및 구조"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 위조지폐범(생성자)와 경찰(구분자)로 비유하면, 위조지폐범은 경찰을 최대한 속이려 하고, 경찰은 위조한 지폐를 최대한 감별하려 한다는 것\n",
    "- 이렇게 위조지폐를 만들고 감별하려는 경쟁을 하게 되면, 서로의 능력이 발전하게 되고 나중에는 위조지폐범은 진짜와 거의 구분할 수 없을 정도로 진짜 같은 위조지폐를 만들게 된다는 것"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GAN구조](./GAN.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 실제 이미지를 주고 구분자(Discriminator)에게 이미지가 진짜임을 판단하게 함\n",
    "2. 생성자(Generator)를 통해 노이즈로부터 임의의 이미지를 만든다.\n",
    "3. 만들어진 이미지를 다시 구분자를 통해 진짜인지를 판단하게 함\n",
    "<hr>\n",
    "이 과정을 통해 생성자를 구분자를 속여 진짜처럼 보이게 하고, 구분자는 생성자를 만든 이미지를 최대한 가짜라고 구분하도록 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN 기본 모델 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-983fe1aa5b25>:6: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/jaehyunlee/.virtualenvs/3mdeeplearning/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/jaehyunlee/.virtualenvs/3mdeeplearning/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../mnist/data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/jaehyunlee/.virtualenvs/3mdeeplearning/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting ../mnist/data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/jaehyunlee/.virtualenvs/3mdeeplearning/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting ../mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/jaehyunlee/.virtualenvs/3mdeeplearning/lib/python3.7/site-packages/tensorflow_core/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"../mnist/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter 설정\n",
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "learning_rate = 0.0002\n",
    "n_hidden = 256\n",
    "n_input = 28*28\n",
    "n_noise = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise]) # 가짜 이미지 생성 용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성자 신경망용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력층\n",
    "G_W1 = tf.Variable(tf.random_normal([n_noise, n_hidden], stddev=0.01))\n",
    "G_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "# 출력층(입력층의 크기와 같아야함)\n",
    "G_W2 = tf.Variable(tf.random_normal([n_hidden, n_input], stddev=0.01))\n",
    "G_b2 = tf.Variable(tf.zeros([n_input]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구분자 신경망용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_W1 = tf.Variable(tf.random_normal([n_input, n_hidden], stddev=0.01))\n",
    "D_b1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "D_W2 = tf.Variable(tf.random_normal([n_hidden,1], stddev=0.01))\n",
    "D_b2 = tf.Variable(tf.zeros([1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성자 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise_z):\n",
    "    hidden = tf.nn.relu(tf.matmul(noise_z, G_W1) + G_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, G_W2) + G_b2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구분자 신경망 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs):\n",
    "    hidden = tf.nn.relu(tf.matmul(inputs, D_W1) + D_b1)\n",
    "    output = tf.nn.sigmoid(tf.matmul(hidden, D_W2) + D_b2)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "무작위 노이즈 생성 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.normal(size=(batch_size, n_noise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = generator(Z)\n",
    "D_gene = discriminator(G) # 생성된 이미지 구분\n",
    "D_real = discriminator(X) # 실제 이미지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 손실값\n",
    "1. 생성자가 만든 이미지를 구분자가 가짜라고 판단하도록 하는 손실값(경찰 학습용)\n",
    "  - 진짜 이미지 판별값(D_real)은 1에 가까워야 하고(진짜라 판별), 가짜 이미지 판별값(D_gene)은 0에 가까워야 함(가짜라 판별)\n",
    "2. 진짜라고 판단하도록 하는 손실값(위조지폐범 학습용)\n",
    "  - 가짜 이미지 판별값(D_gene)를 1에 가깝에 만들기만 하면 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경찰 학습용 손실값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_D = tf.reduce_mean(tf.log(D_real) + tf.log(1 - D_gene)) # D_real과 (1-D_gene) 값을 더한 값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위조지폐범 학습용 손실값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_G = tf.reduce_mean(tf.log(D_gene)) # 해당 값을 1에 가깝게 만들어야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_var_list = [D_W1, D_b1, D_W2, D_b2]\n",
    "G_var_list = [G_W1, G_b1, G_W2, G_b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 변수 최적화 함수 구성(loss값을 최대화 해야함, 음수 부호를 붙여줌)\n",
    "train_D = tf.train.AdamOptimizer(learning_rate).minimize(-loss_D, var_list=D_var_list)\n",
    "train_G = tf.train.AdamOptimizer(learning_rate).minimize(-loss_G, var_list=G_var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0000 D loss : -0.5673 G loss : -2.36\n",
      "Epoch : 0001 D loss : -0.4801 G loss : -2.08\n",
      "Epoch : 0002 D loss : -0.7785 G loss : -1.677\n",
      "Epoch : 0003 D loss : -0.751 G loss : -1.733\n",
      "Epoch : 0004 D loss : -0.7019 G loss : -1.786\n",
      "Epoch : 0005 D loss : -0.6267 G loss : -2.071\n",
      "Epoch : 0006 D loss : -0.7218 G loss : -1.987\n",
      "Epoch : 0007 D loss : -0.5723 G loss : -2.049\n",
      "Epoch : 0008 D loss : -0.835 G loss : -2.045\n",
      "Epoch : 0009 D loss : -0.5594 G loss : -2.077\n",
      "Epoch : 0010 D loss : -0.8304 G loss : -1.925\n",
      "Epoch : 0011 D loss : -0.8206 G loss : -1.84\n",
      "Epoch : 0012 D loss : -0.7875 G loss : -1.876\n",
      "Epoch : 0013 D loss : -0.7518 G loss : -1.705\n",
      "Epoch : 0014 D loss : -0.7852 G loss : -1.746\n",
      "Epoch : 0015 D loss : -0.5862 G loss : -1.908\n",
      "Epoch : 0016 D loss : -0.7883 G loss : -1.944\n",
      "Epoch : 0017 D loss : -0.7935 G loss : -1.963\n",
      "Epoch : 0018 D loss : -0.7496 G loss : -1.883\n",
      "Epoch : 0019 D loss : -0.6518 G loss : -1.862\n",
      "Epoch : 0020 D loss : -0.7265 G loss : -1.765\n",
      "Epoch : 0021 D loss : -1.037 G loss : -1.627\n",
      "Epoch : 0022 D loss : -0.888 G loss : -1.682\n",
      "Epoch : 0023 D loss : -0.7309 G loss : -1.794\n",
      "Epoch : 0024 D loss : -0.8027 G loss : -1.854\n",
      "Epoch : 0025 D loss : -0.8561 G loss : -1.635\n",
      "Epoch : 0026 D loss : -0.7835 G loss : -1.641\n",
      "Epoch : 0027 D loss : -1.021 G loss : -1.367\n",
      "Epoch : 0028 D loss : -0.9825 G loss : -1.59\n",
      "Epoch : 0029 D loss : -0.7512 G loss : -1.8\n",
      "Epoch : 0030 D loss : -0.7091 G loss : -1.708\n",
      "Epoch : 0031 D loss : -1.003 G loss : -1.321\n",
      "Epoch : 0032 D loss : -0.9662 G loss : -1.413\n",
      "Epoch : 0033 D loss : -0.9443 G loss : -1.493\n",
      "Epoch : 0034 D loss : -0.8294 G loss : -1.464\n",
      "Epoch : 0035 D loss : -0.9616 G loss : -1.473\n",
      "Epoch : 0036 D loss : -0.9897 G loss : -1.55\n",
      "Epoch : 0037 D loss : -0.7813 G loss : -1.752\n",
      "Epoch : 0038 D loss : -1.024 G loss : -1.659\n",
      "Epoch : 0039 D loss : -1.098 G loss : -1.411\n",
      "Epoch : 0040 D loss : -0.9843 G loss : -1.458\n",
      "Epoch : 0041 D loss : -0.9861 G loss : -1.387\n",
      "Epoch : 0042 D loss : -0.9048 G loss : -1.817\n",
      "Epoch : 0043 D loss : -0.8389 G loss : -1.533\n",
      "Epoch : 0044 D loss : -1.129 G loss : -1.423\n",
      "Epoch : 0045 D loss : -0.9414 G loss : -1.611\n",
      "Epoch : 0046 D loss : -1.086 G loss : -1.577\n",
      "Epoch : 0047 D loss : -0.8387 G loss : -1.681\n",
      "Epoch : 0048 D loss : -0.8035 G loss : -1.691\n",
      "Epoch : 0049 D loss : -0.935 G loss : -1.388\n",
      "Epoch : 0050 D loss : -0.8563 G loss : -1.563\n",
      "Epoch : 0051 D loss : -0.9913 G loss : -1.73\n",
      "Epoch : 0052 D loss : -0.9675 G loss : -1.43\n",
      "Epoch : 0053 D loss : -1.004 G loss : -1.423\n",
      "Epoch : 0054 D loss : -1.11 G loss : -1.381\n",
      "Epoch : 0055 D loss : -0.8599 G loss : -1.606\n",
      "Epoch : 0056 D loss : -0.9647 G loss : -1.602\n",
      "Epoch : 0057 D loss : -0.8864 G loss : -1.546\n",
      "Epoch : 0058 D loss : -1.112 G loss : -1.285\n",
      "Epoch : 0059 D loss : -0.8827 G loss : -1.685\n",
      "Epoch : 0060 D loss : -1.065 G loss : -1.446\n",
      "Epoch : 0061 D loss : -0.978 G loss : -1.42\n",
      "Epoch : 0062 D loss : -0.8998 G loss : -1.413\n",
      "Epoch : 0063 D loss : -0.9 G loss : -1.655\n",
      "Epoch : 0064 D loss : -1.02 G loss : -1.263\n",
      "Epoch : 0065 D loss : -0.9289 G loss : -1.539\n",
      "Epoch : 0066 D loss : -0.9967 G loss : -1.575\n",
      "Epoch : 0067 D loss : -0.8581 G loss : -1.587\n",
      "Epoch : 0068 D loss : -0.9191 G loss : -1.491\n",
      "Epoch : 0069 D loss : -0.8094 G loss : -1.548\n",
      "Epoch : 0070 D loss : -0.9287 G loss : -1.496\n",
      "Epoch : 0071 D loss : -0.8357 G loss : -1.534\n",
      "Epoch : 0072 D loss : -0.8387 G loss : -1.617\n",
      "Epoch : 0073 D loss : -0.9063 G loss : -1.536\n",
      "Epoch : 0074 D loss : -0.8791 G loss : -1.645\n",
      "Epoch : 0075 D loss : -0.8063 G loss : -1.567\n",
      "Epoch : 0076 D loss : -0.8788 G loss : -1.675\n",
      "Epoch : 0077 D loss : -0.9234 G loss : -1.547\n",
      "Epoch : 0078 D loss : -0.7845 G loss : -1.581\n",
      "Epoch : 0079 D loss : -0.8943 G loss : -1.639\n",
      "Epoch : 0080 D loss : -0.8762 G loss : -1.557\n",
      "Epoch : 0081 D loss : -0.8398 G loss : -1.705\n",
      "Epoch : 0082 D loss : -0.8238 G loss : -1.465\n",
      "Epoch : 0083 D loss : -0.7832 G loss : -1.755\n",
      "Epoch : 0084 D loss : -0.9421 G loss : -1.692\n",
      "Epoch : 0085 D loss : -0.7563 G loss : -1.782\n",
      "Epoch : 0086 D loss : -0.8404 G loss : -1.782\n",
      "Epoch : 0087 D loss : -0.7055 G loss : -1.707\n",
      "Epoch : 0088 D loss : -0.7375 G loss : -1.596\n",
      "Epoch : 0089 D loss : -0.7647 G loss : -1.801\n",
      "Epoch : 0090 D loss : -0.8888 G loss : -1.548\n",
      "Epoch : 0091 D loss : -0.7914 G loss : -1.905\n",
      "Epoch : 0092 D loss : -0.7744 G loss : -1.761\n",
      "Epoch : 0093 D loss : -0.6583 G loss : -2.025\n",
      "Epoch : 0094 D loss : -0.713 G loss : -1.82\n",
      "Epoch : 0095 D loss : -0.7635 G loss : -1.673\n",
      "Epoch : 0096 D loss : -0.7788 G loss : -1.778\n",
      "Epoch : 0097 D loss : -0.8595 G loss : -1.801\n",
      "Epoch : 0098 D loss : -0.6841 G loss : -1.633\n",
      "Epoch : 0099 D loss : -0.8014 G loss : -1.557\n",
      "최적화 완료\n"
     ]
    }
   ],
   "source": [
    "# 학습 반복\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _, loss_val_D = sess.run([train_D, loss_D], feed_dict={X: batch_xs, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G], feed_dict={Z: noise})\n",
    "        \n",
    "    print('Epoch : %04d' %epoch, 'D loss : {:.4}'.format(loss_val_D), 'G loss : {:.4}'.format(loss_val_G))\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Z: noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(1, sample_size, figsize=(sample_size, 1))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[i].set_axis_off()\n",
    "            ax[i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "            \n",
    "        plt.savefig('{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('최적화 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 원하는 숫자 생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_epoch = 100\n",
    "batch_size = 100\n",
    "n_hidden = 256\n",
    "n_input = 28 * 28\n",
    "n_noise = 128\n",
    "n_class = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_input])\n",
    "Y = tf.placeholder(tf.float32, [None, n_class]) # 노이즈와 실제 이미지 각각에 해당하는 숫자를 힌트로 넣어주는 용도\n",
    "Z = tf.placeholder(tf.float32, [None, n_noise])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생성자 신경망 구성(use tf.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(noise, labels):\n",
    "    with tf.variable_scope('generator'):\n",
    "        inputs = tf.concat([noise, labels], 1)\n",
    "        \n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, n_input, activation=tf.nn.sigmoid)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "구분자 신경망 구성(use tf.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(inputs, labels, reuse=None):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        if reuse:\n",
    "            scope.reuse_variables()\n",
    "        inputs = tf.concat([inputs, labels], 1)\n",
    "        hidden = tf.layers.dense(inputs, n_hidden, activation=tf.nn.relu)\n",
    "        output = tf.layers.dense(hidden, 1, activation=None)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "노이즈 생성 함수(균등 분포)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise(batch_size, n_noise):\n",
    "    return np.random.uniform(-1., 1., size=[batch_size, n_noise])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-47a126fc7884>:5: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /Users/jaehyunlee/.virtualenvs/3mdeeplearning/lib/python3.7/site-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n"
     ]
    }
   ],
   "source": [
    "G = generator(Z, Y)\n",
    "D_real = discriminator(X,Y)\n",
    "D_gene = discriminator(G,Y,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수(구분자, 생성자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/jaehyunlee/.virtualenvs/3mdeeplearning/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "loss_D_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_real, labels=tf.ones_like(D_real)))\n",
    "loss_D_gene = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.zeros_like(D_gene)))\n",
    "loss_D = loss_D_real + loss_D_gene # 이 값을 최소화(D_real은 1에 가까워야 하고, D_gene는 0에 가까워야 함)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_gene, labels=tf.ones_like(D_gene)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습모델 구성(tf.get_collection 사용)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_D = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='discriminator')\n",
    "vars_G = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='generator')\n",
    "\n",
    "train_D = tf.train.AdamOptimizer().minimize(loss_D, var_list=vars_D)\n",
    "train_G = tf.train.AdamOptimizer().minimize(loss_G, var_list=vars_G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 0000 D loss : 0.01822 G loss : 7.69\n",
      "Epoch : 0001 D loss : 0.003534 G loss : 10.29\n",
      "Epoch : 0002 D loss : 0.003204 G loss : 10.84\n",
      "Epoch : 0003 D loss : 0.003842 G loss : 8.779\n",
      "Epoch : 0004 D loss : 0.002424 G loss : 9.634\n",
      "Epoch : 0005 D loss : 0.004695 G loss : 10.2\n",
      "Epoch : 0006 D loss : 0.002279 G loss : 10.92\n",
      "Epoch : 0007 D loss : 0.005077 G loss : 8.932\n",
      "Epoch : 0008 D loss : 0.008255 G loss : 8.729\n",
      "Epoch : 0009 D loss : 0.01844 G loss : 7.132\n",
      "Epoch : 0010 D loss : 0.04805 G loss : 6.635\n",
      "Epoch : 0011 D loss : 0.09691 G loss : 6.728\n",
      "Epoch : 0012 D loss : 0.106 G loss : 6.032\n",
      "Epoch : 0013 D loss : 0.1022 G loss : 5.218\n",
      "Epoch : 0014 D loss : 0.1816 G loss : 4.723\n",
      "Epoch : 0015 D loss : 0.3645 G loss : 4.902\n",
      "Epoch : 0016 D loss : 0.2593 G loss : 4.93\n",
      "Epoch : 0017 D loss : 0.4327 G loss : 3.614\n",
      "Epoch : 0018 D loss : 0.428 G loss : 3.459\n",
      "Epoch : 0019 D loss : 0.5278 G loss : 4.15\n",
      "Epoch : 0020 D loss : 0.3549 G loss : 3.897\n",
      "Epoch : 0021 D loss : 0.464 G loss : 4.013\n",
      "Epoch : 0022 D loss : 0.5191 G loss : 3.319\n",
      "Epoch : 0023 D loss : 0.4454 G loss : 3.744\n",
      "Epoch : 0024 D loss : 0.7568 G loss : 3.256\n",
      "Epoch : 0025 D loss : 0.6055 G loss : 3.263\n",
      "Epoch : 0026 D loss : 0.4125 G loss : 2.964\n",
      "Epoch : 0027 D loss : 0.7614 G loss : 2.539\n",
      "Epoch : 0028 D loss : 0.6561 G loss : 2.838\n",
      "Epoch : 0029 D loss : 0.719 G loss : 2.484\n",
      "Epoch : 0030 D loss : 0.5394 G loss : 2.31\n",
      "Epoch : 0031 D loss : 0.8961 G loss : 2.329\n",
      "Epoch : 0032 D loss : 0.7201 G loss : 2.333\n",
      "Epoch : 0033 D loss : 0.7369 G loss : 2.277\n",
      "Epoch : 0034 D loss : 0.8768 G loss : 2.063\n",
      "Epoch : 0035 D loss : 0.7763 G loss : 2.085\n",
      "Epoch : 0036 D loss : 0.6457 G loss : 2.384\n",
      "Epoch : 0037 D loss : 0.6589 G loss : 2.105\n",
      "Epoch : 0038 D loss : 0.6821 G loss : 2.214\n",
      "Epoch : 0039 D loss : 0.8089 G loss : 2.533\n",
      "Epoch : 0040 D loss : 0.7348 G loss : 2.068\n",
      "Epoch : 0041 D loss : 0.6884 G loss : 2.168\n",
      "Epoch : 0042 D loss : 0.7731 G loss : 1.819\n",
      "Epoch : 0043 D loss : 0.6951 G loss : 1.836\n",
      "Epoch : 0044 D loss : 0.8753 G loss : 2.335\n",
      "Epoch : 0045 D loss : 0.8097 G loss : 1.974\n",
      "Epoch : 0046 D loss : 0.773 G loss : 1.814\n",
      "Epoch : 0047 D loss : 0.8082 G loss : 2.004\n",
      "Epoch : 0048 D loss : 0.6374 G loss : 2.054\n",
      "Epoch : 0049 D loss : 0.7309 G loss : 2.358\n",
      "Epoch : 0050 D loss : 0.7524 G loss : 2.344\n",
      "Epoch : 0051 D loss : 0.6908 G loss : 1.839\n",
      "Epoch : 0052 D loss : 0.7694 G loss : 1.944\n",
      "Epoch : 0053 D loss : 0.5481 G loss : 2.352\n",
      "Epoch : 0054 D loss : 0.8618 G loss : 2.183\n",
      "Epoch : 0055 D loss : 0.774 G loss : 1.902\n",
      "Epoch : 0056 D loss : 0.772 G loss : 2.261\n",
      "Epoch : 0057 D loss : 0.7166 G loss : 2.039\n",
      "Epoch : 0058 D loss : 0.9775 G loss : 1.869\n",
      "Epoch : 0059 D loss : 0.7395 G loss : 1.726\n",
      "Epoch : 0060 D loss : 0.7645 G loss : 1.992\n",
      "Epoch : 0061 D loss : 0.8358 G loss : 1.744\n",
      "Epoch : 0062 D loss : 0.7434 G loss : 1.81\n",
      "Epoch : 0063 D loss : 0.5959 G loss : 2.376\n",
      "Epoch : 0064 D loss : 0.69 G loss : 2.172\n",
      "Epoch : 0065 D loss : 0.7509 G loss : 1.875\n",
      "Epoch : 0066 D loss : 0.7897 G loss : 2.135\n",
      "Epoch : 0067 D loss : 0.6789 G loss : 2.169\n",
      "Epoch : 0068 D loss : 0.8001 G loss : 2.267\n",
      "Epoch : 0069 D loss : 0.714 G loss : 2.208\n",
      "Epoch : 0070 D loss : 0.8665 G loss : 1.902\n",
      "Epoch : 0071 D loss : 0.921 G loss : 2.095\n",
      "Epoch : 0072 D loss : 0.8104 G loss : 2.057\n",
      "Epoch : 0073 D loss : 0.5946 G loss : 2.363\n",
      "Epoch : 0074 D loss : 0.7426 G loss : 1.964\n",
      "Epoch : 0075 D loss : 0.8846 G loss : 1.892\n",
      "Epoch : 0076 D loss : 0.7884 G loss : 1.906\n",
      "Epoch : 0077 D loss : 0.7332 G loss : 1.695\n",
      "Epoch : 0078 D loss : 0.8046 G loss : 1.803\n",
      "Epoch : 0079 D loss : 0.8473 G loss : 2.013\n",
      "Epoch : 0080 D loss : 0.6185 G loss : 2.0\n",
      "Epoch : 0081 D loss : 0.6966 G loss : 1.865\n",
      "Epoch : 0082 D loss : 0.9537 G loss : 1.697\n",
      "Epoch : 0083 D loss : 0.6744 G loss : 2.389\n",
      "Epoch : 0084 D loss : 0.8254 G loss : 1.96\n",
      "Epoch : 0085 D loss : 0.7498 G loss : 2.131\n",
      "Epoch : 0086 D loss : 0.6924 G loss : 1.745\n",
      "Epoch : 0087 D loss : 0.8357 G loss : 1.97\n",
      "Epoch : 0088 D loss : 0.9523 G loss : 1.999\n",
      "Epoch : 0089 D loss : 0.7796 G loss : 1.959\n",
      "Epoch : 0090 D loss : 0.8432 G loss : 1.818\n",
      "Epoch : 0091 D loss : 0.6697 G loss : 2.034\n",
      "Epoch : 0092 D loss : 0.7112 G loss : 2.175\n",
      "Epoch : 0093 D loss : 0.8017 G loss : 1.777\n",
      "Epoch : 0094 D loss : 0.8337 G loss : 1.763\n",
      "Epoch : 0095 D loss : 0.7023 G loss : 2.07\n",
      "Epoch : 0096 D loss : 0.6726 G loss : 1.868\n",
      "Epoch : 0097 D loss : 0.7062 G loss : 2.204\n",
      "Epoch : 0098 D loss : 0.656 G loss : 2.248\n",
      "Epoch : 0099 D loss : 0.8429 G loss : 1.851\n",
      "최적화 완료\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "total_batch = int(mnist.train.num_examples / batch_size)\n",
    "loss_val_D, loss_val_G = 0, 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    for i in range(total_batch):\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        noise = get_noise(batch_size, n_noise)\n",
    "        \n",
    "        _, loss_val_D = sess.run([train_D, loss_D], feed_dict={X: batch_xs, Y: batch_ys, Z: noise})\n",
    "        _, loss_val_G = sess.run([train_G, loss_G], feed_dict={Y: batch_ys, Z: noise})\n",
    "        \n",
    "    print('Epoch : %04d' %epoch, 'D loss : {:.4}'.format(loss_val_D), 'G loss : {:.4}'.format(loss_val_G))\n",
    "    \n",
    "    if epoch == 0 or (epoch + 1) % 10 == 0:\n",
    "        sample_size = 10\n",
    "        noise = get_noise(sample_size, n_noise)\n",
    "        samples = sess.run(G, feed_dict={Y: mnist.test.labels[:sample_size], Z: noise})\n",
    "        \n",
    "        fig, ax = plt.subplots(2, sample_size, figsize=(sample_size, 2))\n",
    "        \n",
    "        for i in range(sample_size):\n",
    "            ax[0][i].set_axis_off()\n",
    "            ax[1][i].set_axis_off()\n",
    "            \n",
    "            ax[0][i].imshow(np.reshape(mnist.test.images[i], (28,28)))\n",
    "            ax[1][i].imshow(np.reshape(samples[i], (28, 28)))\n",
    "            \n",
    "        plt.savefig('{}.png'.format(str(epoch).zfill(3)), bbox_inches='tight')\n",
    "        plt.close(fig)\n",
    "print('최적화 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고 : https://goo.gl/ZvSvtm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
